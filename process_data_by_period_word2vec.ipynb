{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pycountry\n",
    "import re\n",
    "import ast\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from datetime import datetime\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import country_converter as coco\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('un_general_debates_extended.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = coco.CountryConverter()\n",
    "country_name_short=data['country_name_short']\n",
    "un_region_names = cc.convert(names = country_name_short, to = 'UNregion')\n",
    "\n",
    "data.loc[:,'UN_region']=un_region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text data to lower case (for easier analysis)\n",
    "data.loc[:,'text'] = data['text'].str.lower()\n",
    "# Remove unusual symbols from description\n",
    "def clean(s):    \n",
    "    # Remove any tags:\n",
    "    cleaned = re.sub(r\"(?s)<.?>\", \" \", s)\n",
    "    # Keep only regular chars:\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9(),*!?\\'\\`]\", \" \", cleaned)\n",
    "    # Remove unicode chars\n",
    "    cleaned = re.sub(\"\\\\\\\\u(.){4}\", \" \", cleaned)\n",
    "    # Remove things between brackets\n",
    "    cleaned = re.sub(\"\\[.*?\\]\", \" \", cleaned)\n",
    "\n",
    "    return cleaned.strip()\n",
    "\n",
    "# clean text\n",
    "data.loc[:,'text'] = data.text.apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data with null value in year column\n",
    "data = data[data['year'].notnull()]\n",
    "\n",
    "# # drop session column -- provides no information\n",
    "# data = data.drop(['session'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import ToktokTokenizer\n",
    "import string\n",
    "from sklearn.feature_extraction import text\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "#====================================================================================#\n",
    "#\n",
    "# Description:\n",
    "# A script to preprocess political texts, with procedural stop word removal.\n",
    "# For more information, see www.github.com/lrheault/partyembed\n",
    "#\n",
    "# Usage:\n",
    "# python3 preprocess.py [USA/Canada/UK]\n",
    "#\n",
    "# @author: L. Rheault\n",
    "#\n",
    "#====================================================================================#\n",
    "\n",
    "\n",
    "tk = ToktokTokenizer()\n",
    "# For replacement of contractions.\n",
    "contractions = {\"you'd\": 'you would', \"he'd\": 'he would', \"she's\": 'she is', \"where'd\": 'where did', \"might've\": 'might have', \"he'll\": 'he will', \"they'll\": 'they will',  \"mightn't\": 'might not', \"you'd've\": 'you would have', \"shan't\": 'shall not', \"it'll\": 'it will', \"mayn't\": 'may not', \"couldn't\": 'could not', \"they'd\": 'they would', \"so've\": 'so have', \"needn't've\": 'need not have', \"they'll've\": 'they will have', \"it's\": 'it is', \"haven't\": 'have not', \"didn't\": 'did not', \"y'all'd\": 'you all would', \"needn't\": 'need not', \"who'll\": 'who will', \"wouldn't've\": 'would not have', \"when's\": 'when is', \"will've\": 'will have', \"it'd've\": 'it would have', \"what'll\": 'what will', \"that'd've\": 'that would have', \"y'all're\": 'you all are', \"let's\": 'let us', \"where've\": 'where have', \"o'clock\": 'oclock', \"when've\": 'when have', \"what're\": 'what are', \"should've\": 'should have', \"you've\": 'you have', \"they're\": 'they are', \"aren't\": 'are not', \"they've\": 'they have', \"it'd\": 'it would', \"i'll've\": 'i will have', \"they'd've\": 'they would have', \"you'll've\": 'you will have', \"wouldn't\": 'would not', \"we'd\": 'we would', \"hadn't've\": 'had not have', \"weren't\": 'were not', \"i'd\": 'i would', \"must've\": 'must have', \"what's\": 'what is', \"mustn't've\": 'must not have', \"what'll've\": 'what will have', \"ain't\": 'aint', \"doesn't\": 'does not', \"we'll\": 'we will', \"i'd've\": 'i would have', \"we've\": 'we have', \"oughtn't\": 'ought not', \"you're\": 'you are', \"who'll've\": 'who will have', \"shouldn't\": 'should not', \"can't've\": 'cannot have', \"i've\": 'i have', \"couldn't've\": 'could not have', \"why've\": 'why have', \"what've\": 'what have', \"can't\": 'cannot', \"don't\": 'do not', \"that'd\": 'that would', \"who's\": 'who is', \"would've\": 'would have', \"there'd\": 'there would', \"shouldn't've\": 'should not have', \"y'all\": 'you all', \"mustn't\": 'must not', \"she'll\": 'she will', \"hadn't\": 'had not', \"won't've\": 'will not have', \"why's\": 'why is', \"'cause\": 'because', \"wasn't\": 'was not', \"shan't've\": 'shall not have', \"ma'am\": 'madam', \"hasn't\": 'has not', \"to've\": 'to have', \"how'll\": 'how will', \"oughtn't've\": 'ought not have', \"he'll've\": 'he will have', \"we'd've\": 'we would have', \"won't\": 'will not', \"could've\": 'could have', \"isn't\": 'is not', \"she'll've\": 'she will have', \"we'll've\": 'we will have', \"you'll\": 'you will', \"who've\": 'who have', \"there's\": 'there is', \"y'all've\": 'you all have', \"we're\": 'we are', \"i'll\": 'i will', \"i'm\": 'i am', \"how's\": 'how is', \"she'd've\": 'she would have', \"sha'n't\": 'shall not', \"there'd've\": 'there would have', \"he's\": 'he is', \"it'll've\": 'it will have', \"that's\": 'that is', \"y'all'd've\": 'you all would have', \"he'd've\": 'he would have', \"how'd\": 'how did', \"where's\": 'where is', \"so's\": 'so as', \"she'd\": 'she would', \"mightn't've\": 'might not have'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_stopwords = ['nations','year','believe','important','assembly','secretary',\n",
    "                'conference', 'like', 'way', 'state', 'resolution', \n",
    "                'government', 'make', 'role', \n",
    "                'united nation', 'united nations', 'general assembly', 'republic of', 'secretary general', 'the world', \n",
    "                'ibid', 'security council', 'member state', 'country', 'must', 'many'] + list(text.ENGLISH_STOP_WORDS)\n",
    "\n",
    "def tokenize_text(text, stopwords=un_stopwords):\n",
    "    text = reduce(lambda a, kv: a.replace(*kv), contractions.items(), text.lower())\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    tokens = tk.tokenize(text)\n",
    "    \n",
    "    tokens = [token for token in tokens if len(token) > 3 and\n",
    "             token not in stopwords and not token.isdigit()]\n",
    "    \n",
    "    #remove inflectional endings and get the root word (lemma):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "data['clean_text'] = data['text'].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Create a Phrases model to detect common bigrams\n",
    "bigram_model = Phrases(data['clean_text'].values, min_count=3, threshold=2)\n",
    "\n",
    "# Convert the bigrams into single tokens\n",
    "phraser = Phraser(bigram_model)\n",
    "processed_sentences = [phraser[sentence] for sentence in data['clean_text'].values]\n",
    "\n",
    "print(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag(row):\n",
    "    country= row[\"UN_region\"].replace(' ', '_')\n",
    "    if row[\"year\"]<1990:\n",
    "        time_stamp_cold_war='before'\n",
    "    else:\n",
    "        time_stamp_cold_war='after'\n",
    "    return country + \"_\" + time_stamp_cold_war\n",
    "\n",
    "data[\"tag\"] = data.apply(lambda row: create_tag(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df=data.groupby('year')['clean_text'].apply(sum).reset_index()\n",
    "dict_tag_grouped_text = dict(zip(grouped_df['year'], grouped_df['clean_text']))\n",
    "data.loc[:,'clean_text_merged'] = data['year'].map(lambda x: dict_tag_grouped_text.get(x, None))\n",
    "data_year=data[['year', 'clean_text_merged']]\n",
    "data_year.drop_duplicates(subset=['year'], keep='first', inplace=True)\n",
    "data_year.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df=data.groupby('tag')['clean_text'].apply(sum).reset_index()\n",
    "# dict_tag_grouped_text = dict(zip(grouped_df['tag'], grouped_df['clean_text']))\n",
    "# data.loc[:,'clean_text_merged'] = data['tag'].map(lambda x: dict_tag_grouped_text.get(x, None))\n",
    "# data=data[['year', 'clean_text', 'tag', 'UN_region', 'clean_text_merged']]\n",
    "# data.drop_duplicates(subset=['tag'], keep='first', inplace=True)\n",
    "# data.reset_index(drop=True, inplace=True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the occurrences of each word in the entire corpus\n",
    "word_counts = Counter(word for text_list in data_year['clean_text_merged'] for word in text_list)\n",
    "\n",
    "# Filter out words that appear less than three times\n",
    "words_to_keep = {word for word, count in word_counts.items() if count >= 3}\n",
    "\n",
    "# Remove the filtered words from the DataFrame\n",
    "def filter_words(text_list):\n",
    "    return [word for word in text_list if word in words_to_keep]\n",
    "\n",
    "# Apply the filter_words function to each row of the 'text' column\n",
    "data_year['clean_text_merged'] = data_year['clean_text_merged'].apply(filter_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(data_year['clean_text_merged'].iloc[k], [data_year['year'].iloc[k]]) for k in tqdm(range(len(data_year)))]\n",
    "\n",
    "model = Doc2Vec(documents, workers=10, vector_size=300, min_count=3, window=10, epochs=5)\n",
    "fname = f\"doc2vec_whole_corpus_year\"\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed = data_year #why do you apply it on year?\n",
    "data_embed[f'text_embedding'] = data_embed['year'].progress_apply(lambda x: model.dv[x])\n",
    "data_embed.to_csv(f'df_text_embedding_year.csv')\n",
    "#data_embed['clean_text_merged']=data_embed['clean_text_merged'].apply(lambda lst: [item for item in lst if item in set(model.wv.index_to_key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_embed = data[['tag']].drop_duplicates()\n",
    "# data_embed[f'text_embedding'] = data_embed['tag'].progress_apply(lambda x: model.dv[x])\n",
    "# data_embed['clean_text']=data['clean_text']\n",
    "# data_embed['UN_region']=data['UN_region']\n",
    "# data_embed['before/after cold war']=data['year'].apply(lambda x: 'before' if x<1990 else 'after')\n",
    "# data_embed.to_csv(f'df_text_embedding_un_region_cold_war.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed=pd.read_csv('df_text_embedding_un_region_cold_war.csv', index_col=0)\n",
    "def add_comas(row):\n",
    "    string_representation=row['text_embedding']\n",
    "    string_without_whitespace = '['+string_representation[2:]\n",
    "    string_with_commas = ', '.join(string_without_whitespace.split())\n",
    "    return string_with_commas \n",
    "\n",
    "data_embed['text_embedding']= data_embed.apply(lambda row: add_comas(row), axis=1)\n",
    "data_embed['text_embedding'] = data_embed['text_embedding'].apply(ast.literal_eval)\n",
    "data_embed ['clean_text'] = data_embed['clean_text'].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_COL = {\n",
    "    'Australia and New Zealand': 'blue',\n",
    "    'Caribbean': 'green',\n",
    "    'Central America': 'red',\n",
    "    'Central Asia': 'orange',\n",
    "    'Eastern Africa': 'purple',\n",
    "    'Eastern Asia': 'cyan',\n",
    "    'Eastern Europe': 'magenta',\n",
    "    'Melanesia': 'yellow',\n",
    "    'Micronesia': 'lime',\n",
    "    'Middle Africa': 'pink',\n",
    "    'Northern Africa': 'brown',\n",
    "    'Northern America': 'teal',\n",
    "    'Northern Europe': 'olive',\n",
    "    'Polynesia': 'navy',\n",
    "    'South America': 'maroon',\n",
    "    'South-eastern Asia': 'indigo',\n",
    "    'Southern Africa': 'gold',\n",
    "    'Southern Asia': 'tan',\n",
    "    'Southern Europe': 'skyblue',\n",
    "    'Western Africa': 'violet',\n",
    "    'Western Asia': 'crimson',\n",
    "    'Western Europe': 'salmon'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unguided analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed=pd.read_csv('df_text_embedding_year.csv', index_col=0)\n",
    "\n",
    "data_embed['text_embedding']= data_embed.apply(lambda row: add_comas(row), axis=1)\n",
    "data_embed['text_embedding'] = data_embed['text_embedding'].apply(ast.literal_eval)\n",
    "data_embed['clean_text_merged']=data_embed['clean_text_merged'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Définir la taille du graphique\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Function to perform PCA\n",
    "def perform_pca(data_embed):\n",
    "    # Convert the embeddings to a numpy array\n",
    "    X = np.array(data_embed['text_embedding'].tolist())\n",
    "    \n",
    "    # Standardize the data\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return pca, X_pca\n",
    "\n",
    "# Perform PCA on df_main\n",
    "pca_main, X_pca_main = perform_pca(data_embed)\n",
    "\n",
    "def project_pca(df_subset):\n",
    "    # X should be a numpy array of shape (n_samples, n_features)\n",
    "    X_subset = np.array(df_subset['text_embedding'].tolist())\n",
    "    \n",
    "    # Step 2: Standardize the data\n",
    "    X_scaled_subset = scaler.transform(X_subset)\n",
    "    X_pca_subset = pca_main.transform(X_scaled_subset)\n",
    "\n",
    "    return X_pca_subset\n",
    "\n",
    "pca_comp_1={}\n",
    "pca_comp_2={}\n",
    "for year in set(data_embed['year'].values):\n",
    "    # Filtrer les données pour la famille actuelle\n",
    "    data = data_embed.loc[data_embed['year']==year].reset_index()\n",
    "    \n",
    "    # Obtenir les projections 2D pour les embeddings de la famille actuelle\n",
    "    projections_2D = project_pca(data)\n",
    "    pca_comp_1[year]=(projections_2D[:,0])\n",
    "    pca_comp_2[year]=(projections_2D[:,1])\n",
    "    # Créer un graphique en nuage de points pour la famille actuelle\n",
    "    ax.scatter(projections_2D[:,0], projections_2D[:,1], alpha=0.7, label=year)\n",
    "    \n",
    "data_embed['PCA_component1']=data_embed['year'].map(pca_comp_1)\n",
    "data_embed['PCA_component2']=data_embed['year'].map(pca_comp_2)\n",
    "'''\n",
    "# Parcourir les familles et créer un graphique en nuage de points pour chaque famille\n",
    "for un_region in set(data_embed['UN_region'].values):\n",
    "    # Filtrer les données pour la famille actuelle\n",
    "    data = data_embed.loc[data_embed['UN_region']==un_region].reset_index()\n",
    "    \n",
    "    # Obtenir les projections 2D pour les embeddings de la famille actuelle\n",
    "    projections_2D = project_pca(data)\n",
    "    \n",
    "    # Créer un graphique en nuage de points pour la famille actuelle\n",
    "    ax.scatter(projections_2D[:,0], projections_2D[:,1], alpha=0.7, label=un_region)\n",
    "    \n",
    "    # Ajouter l'année sur chaque point, avec une barre de séparation en cas de chevauchement\n",
    "    for i, year in enumerate(data['before/after cold war']):\n",
    "        # Vérifier s'il y a un chevauchement avec l'année précédente\n",
    "        if i > 0 and data['before/after cold war'][i-1] == year:\n",
    "            # Ajouter une barre de séparation\n",
    "            ax.plot([projections_2D[i-1,0], projections_2D[i,0]], [projections_2D[i-1,1], projections_2D[i,1]], color=FAMILY_COL[family], linestyle='-', linewidth=0.5)\n",
    "            # Ajouter l'année avec une légère décalage vertical\n",
    "            ax.text(projections_2D[i,0], projections_2D[i,1]+0.8, year, ha='center', va='bottom')\n",
    "        else:\n",
    "            # Ajouter l'année sans barre de séparation\n",
    "            ax.text(projections_2D[i,0], projections_2D[i,1]+0.5, year, ha='center', va='bottom')'''\n",
    "\n",
    "ax.set_title('PCA of Embedded Speeches', fontsize=14)\n",
    "\n",
    "# Définir la taille de la police de caractère pour les étiquettes d'axes\n",
    "ax.set_xlabel('Component 1', fontsize=12)\n",
    "ax.set_ylabel('Component 2', fontsize=12)\n",
    "\n",
    "# Ajouter une légende\n",
    "ax.legend()\n",
    "plt.savefig('PCA_data_by_region.png')\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la taille du graphique\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(data_embed['year'],data_embed['PCA_component1'])\n",
    "ax.set_title('Component 1 of PCA throughout the years (whole corpus)', fontsize=14)\n",
    "\n",
    "# Définir la taille de la police de caractère pour les étiquettes d'axes\n",
    "ax.set_xlabel('year', fontsize=12)\n",
    "ax.set_ylabel('Component 1', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la taille du graphique\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(data_embed['year'],data_embed['PCA_component2'])\n",
    "ax.set_title('Component 2 of PCA throughout the years (whole corpus)', fontsize=14)\n",
    "\n",
    "# Définir la taille de la police de caractère pour les étiquettes d'axes\n",
    "ax.set_xlabel('year', fontsize=12)\n",
    "ax.set_ylabel('Component 2', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean values of component 1 and component 2\n",
    "mean_component1 = np.mean(X_pca_main[:, 0])\n",
    "mean_component2 = np.mean(X_pca_main[:, 1])\n",
    "\n",
    "# Identify the indices of data points in the bottom left quadrant\n",
    "bottom_left_indices = np.where((X_pca_main[:, 0] < mean_component1) & (X_pca_main[:, 1] < mean_component2))[0]\n",
    "\n",
    "# Retrieve the corresponding words from df_main based on the indices\n",
    "words_bottom_left = data_embed.iloc[bottom_left_indices]['clean_text_merged'].values\n",
    "\n",
    "flattened_words_bottom_left = [word for sublist in words_bottom_left for word in sublist]\n",
    "words_bottom_left=list(set(flattened_words_bottom_left))\n",
    "\n",
    "# Sort these words based on their distance from the bottom left in the PCA plot\n",
    "min_component1 = np.min(X_pca_main[:, 0])\n",
    "min_component2 = np.min(X_pca_main[:, 1])\n",
    "\n",
    "bottom_left_point = (min_component1, min_component2)\n",
    "\n",
    "distances_from_bottom_left = np.sqrt((X_pca_main[:, 0] - bottom_left_point[0])**2 + (X_pca_main[:, 1] - bottom_left_point[1])**2)\n",
    "\n",
    "# Sort the words based on their distances from the origin\n",
    "sorted_words_bottom_left = [word for _, word in sorted(zip(distances_from_bottom_left[bottom_left_indices], words_bottom_left))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the indices of data points in the top left quadrant\n",
    "top_left_indices = np.where((X_pca_main[:, 0] < mean_component1) & (X_pca_main[:, 1] > mean_component2))[0]\n",
    "\n",
    "# Retrieve the corresponding words from df_main based on the indices\n",
    "words_top_left = data_embed.iloc[top_left_indices]['clean_text_merged'].values\n",
    "# Flatten the list of lists\n",
    "flattened_words_top_left = [word for sublist in words_top_left for word in sublist]\n",
    "\n",
    "# Get unique words\n",
    "words_top_left = list(set(flattened_words_top_left))\n",
    "\n",
    "# Sort these words based on their distance from the top left in the PCA plot\n",
    "min_component1 = np.min(X_pca_main[:, 0])\n",
    "max_component2 = np.min(X_pca_main[:, 1])\n",
    "\n",
    "top_left_point = (min_component1, max_component2)\n",
    "\n",
    "distances_from_top_left = np.sqrt((X_pca_main[:, 0] - top_left_point[0])**2 + (X_pca_main[:, 1] - top_left_point[1])**2)\n",
    "\n",
    "# Sort the words based on their distances from the origin\n",
    "sorted_words_top_left = [word for _, word in sorted(zip(distances_from_top_left[top_left_indices], words_top_left))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the indices of data points in the top right quadrant\n",
    "top_right_indices = np.where((X_pca_main[:, 0] > mean_component1) & (X_pca_main[:, 1] > mean_component2))[0]\n",
    "\n",
    "# Retrieve the corresponding words from df_main based on the indices\n",
    "words_top_right = data_embed.iloc[top_right_indices]['clean_text_merged'].values\n",
    "\n",
    "# Flatten the list of lists\n",
    "flattened_words_top_right = [word for sublist in words_top_right for word in sublist]\n",
    "\n",
    "# Get unique words\n",
    "words_top_right = list(set(flattened_words_top_right))\n",
    "\n",
    "# Sort these words based on their distance from the top right in the PCA plot\n",
    "max_component1 = np.max(X_pca_main[:, 0])\n",
    "max_component2 = np.max(X_pca_main[:, 1])\n",
    "\n",
    "top_right_point = (max_component1, max_component2)\n",
    "\n",
    "distances_from_top_right = np.sqrt((X_pca_main[:, 0] - top_right_point[0])**2 + (X_pca_main[:, 1] - top_right_point[1])**2)\n",
    "\n",
    "# Sort the words based on their distances from the origin\n",
    "sorted_words_top_right = [word for _, word in sorted(zip(distances_from_top_right[top_right_indices], words_top_right))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the indices of data points in the bottom right quadrant\n",
    "bottom_right_indices = np.where((X_pca_main[:, 0] > mean_component1) & (X_pca_main[:, 1] < mean_component2))[0]\n",
    "\n",
    "# Retrieve the corresponding words from df_main based on the indices\n",
    "words_bottom_right = data_embed.iloc[bottom_right_indices]['clean_text_merged'].values\n",
    "\n",
    "# Flatten the list of lists\n",
    "flattened_words_bottom_right = [word for sublist in words_bottom_right for word in sublist]\n",
    "\n",
    "# Get unique words\n",
    "words_bottom_right = list(set(flattened_words_bottom_right))\n",
    "\n",
    "# Sort these words based on their distance from the bottom right in the PCA plot\n",
    "max_component1 = np.max(X_pca_main[:, 0])\n",
    "min_component2 = np.min(X_pca_main[:, 1])\n",
    "\n",
    "bottom_right_point = (max_component1, min_component2)\n",
    "\n",
    "distances_from_bottom_right = np.sqrt((X_pca_main[:, 0] - bottom_right_point[0])**2 + (X_pca_main[:, 1] - bottom_right_point[1])**2)\n",
    "\n",
    "# Sort the words based on their distances from the origin\n",
    "sorted_words_bottom_right = [word for _, word in sorted(zip(distances_from_bottom_right[bottom_right_indices], words_bottom_right))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now print the words defining the axes\n",
    "print(\"Words closest to the bottom left quadrant of PCA plot:\", sorted_words_bottom_left[:5])\n",
    "print(\"Words closest to the bottom right quadrant of PCA plot:\", sorted_words_bottom_right[:5])\n",
    "print(\"Words closest to the top right quadrant of PCA plot:\", sorted_words_top_right[:5])\n",
    "print(\"Words closest to the top left quadrant of PCA plot:\", sorted_words_top_left[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = data_embed['clean_text_merged'].values\n",
    "\n",
    "# Seed words\n",
    "\n",
    "territorial_terms = ['bloc', 'region', 'civilisation', 'country', 'nation']\n",
    "functional_terms = ['class', 'ideology', 'inequality', 'group', 'social']\n",
    "\n",
    "seed_words = territorial_terms + functional_terms\n",
    "\n",
    "# Calculate average vector for seed words\n",
    "territorial_seed_vectors = sum([model.wv[word] for word in filtered_TERR_FUNC_LEXICON[0] if word in model.wv.index_to_key]) / len(territorial_terms)\n",
    "functional_seed_vectors = sum([model.wv[word] for word in filtered_TERR_FUNC_LEXICON[1] if word in model.wv.index_to_key]) / len(functional_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_row(row, ref_vector):\n",
    "    # Extract vectors for words that are in the model's vocabulary\n",
    "    word_vectors = np.array(\n",
    "        [model.wv[word] for word in row if word in model.wv.index_to_key]\n",
    "    )\n",
    "    # Calculate cosine similarity for the word vectors against the reference vector\n",
    "    similarities = cosine_similarity(word_vectors, [ref_vector])\n",
    "    # Compute the average cosine similarity\n",
    "    average_similarity = np.mean(similarities)\n",
    "    return average_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between average embeddings and all embedded speeches\n",
    "\n",
    "data_embed['territorial_similarity'] = data_embed['clean_text_merged'].progress_apply(lambda x: cosine_similarity_row(x, territorial_seed_vectors))\n",
    "data_embed['functional_similarity'] = data_embed['clean_text_merged'].progress_apply(lambda x: cosine_similarity_row(x, functional_seed_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_embed['year'], data_embed['functional_similarity'], marker='o', label='functionality')\n",
    "plt.scatter(data_embed['year'], data_embed['territorial_similarity'], marker='o', color='r', label='territoriality')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Plot of functionality and territoriality depending on years')\n",
    "\n",
    "# Add labels to the axes (optional)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('functionality and territoriality')\n",
    "\n",
    "plt.legend()\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate semantic similarity between each word in the vocabulary and seed words\n",
    "word_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "word_similarities = {}\n",
    "\n",
    "for word, vector in word_vectors.items():\n",
    "    territorial_similarity = cosine_similarity([vector], [territorial_seed_vectors])[0][0]\n",
    "    functional_similarity = cosine_similarity([vector], [functional_seed_vectors])[0][0]\n",
    "    word_similarities[word] = (territorial_similarity, functional_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_similarities = sorted(word_similarities.items(), key=lambda x: (x[1][0]), reverse=True)\n",
    "top_20_words_territorial = sorted_word_similarities[:20]\n",
    "\n",
    "top_territorial=[word for word, (func_sim, terr_sim) in top_20_words_territorial[:10]]\n",
    "\n",
    "print(\"Top 10 words related to high territorial terms:\", top_territorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_similarities = sorted(word_similarities.items(), key=lambda x: (x[1][1]), reverse=True)\n",
    "top_20_words_functional = sorted_word_similarities[:20]\n",
    "\n",
    "top_functional=[word for word, (func_sim, terr_sim) in top_20_words_functional[:10]]\n",
    "\n",
    "print(\"Top 10 words related to high functional terms:\", top_functional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided projections according to the git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "\n",
    "territorial_terms = ['bloc', 'region', 'civilisation', 'country', 'nation']\n",
    "functional_terms = ['class', 'ideology', 'inequality', 'group', 'social']\n",
    "\n",
    "def topic_vectors(topicwords, model, n=20):\n",
    "    M = model.vector_size\n",
    "    centroids = []\n",
    "\n",
    "    # Compute the centroid for each topic word and store them in a list\n",
    "    for topicword in topicwords:\n",
    "        sims = model.wv.most_similar(topicword, topn=n)\n",
    "        simw = [topicword] + [w for w, s in sims]\n",
    "        zsim = np.zeros((n + 1, M))\n",
    "        for i, w in enumerate(simw):\n",
    "            zsim[i, :] = model.wv[w]\n",
    "        centroids.append(zsim.mean(axis=0))\n",
    "\n",
    "    # Calculate the average centroid across all topic words\n",
    "    overall_centroid = np.mean(centroids, axis=0)\n",
    "    return overall_centroid\n",
    "\n",
    "def bootstrap_topic_vectors(topicwords, model, n=20, sims=1000):\n",
    "    M = model.vector_size\n",
    "    boot_results = np.zeros((sims, M))\n",
    "    \n",
    "    # Get all topic words to be used in bootstrapping\n",
    "    topic_words = []\n",
    "    for topicword in topicwords:\n",
    "        expanded_word_list = model.wv.most_similar(topicword, topn=n - 1)\n",
    "        topic_words.extend([topicword] + [w for w, s in expanded_word_list])\n",
    "    \n",
    "    # Perform bootstrapping to calculate centroids\n",
    "    for s in range(sims):\n",
    "        boot_sample = np.random.choice(topic_words, size=n, replace=True)\n",
    "        zsim = np.zeros((n, M))\n",
    "        for i, w in enumerate(boot_sample):\n",
    "            zsim[i, :] = model.wv[w]\n",
    "        boot_results[s, :] = zsim.mean(axis=0)\n",
    "\n",
    "    return boot_results\n",
    "\n",
    "def cos_sim(speech, topic, boot=True, sims=1000):\n",
    "    P = speech.shape[0]\n",
    "    if boot:\n",
    "        C = cosine_similarity(speech, topic)\n",
    "        m = np.mean(C, axis=1)\n",
    "        ci = np.percentile(C, q=[2.5, 97.5], axis=1)\n",
    "        return m.tolist(), ci[0].tolist(), ci[1].tolist()\n",
    "    else:\n",
    "        return cosine_similarity(speech, topic).tolist()\n",
    "\n",
    "def issue_ownership(model, topicwords, infer_vector=True, t_size=20, boot=True, smooth=True):\n",
    "    # Get the average centroid for the list of topic words\n",
    "    if infer_vector:\n",
    "        if boot:\n",
    "            t = bootstrap_topic_vectors(topicwords, model, n=t_size, sims=1000)\n",
    "        else:\n",
    "            t = topic_vectors(topicwords, model, n=t_size)\n",
    "    else:\n",
    "        raise ValueError(\"Either topic words or topic vectors must be provided\")\n",
    "\n",
    "    res = fit(model, t, smooth=smooth, boot=boot)\n",
    "    return res\n",
    "\n",
    "def fit(model, topic_vector, smooth=False, boot=True):\n",
    "    M = model.vector_size\n",
    "    years = sorted(list(set(data_embed['year'].values)))\n",
    "    P = len(years)\n",
    "    z = np.zeros((P, M))\n",
    "\n",
    "    # Populate `z` with document vectors for each year\n",
    "    for i, year in enumerate(years):\n",
    "        doc_index = next((index for (index, (doc, tag)) in enumerate(documents) if tag == [year]), None)\n",
    "        if doc_index is not None:\n",
    "            z[i, :] = model.dv[doc_index]\n",
    "    \n",
    "    # Calculate similarity and return results\n",
    "    C = cos_sim(z, topic_vector, boot=boot)\n",
    "    res = pd.DataFrame({'year': years, 'similarity': C[0]})\n",
    "\n",
    "    if smooth:\n",
    "        res = res.rolling(window=10, center=False).mean()\n",
    "        res['year'] = years\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = model.wv.most_similar('ideology', topn = 10)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_territorial_sim=issue_ownership(model, topicwords=filtered_TERR_FUNC_LEXICON[0], smooth=False, boot=True)\n",
    "plt.plot(corpus_territorial_sim['year'], corpus_territorial_sim['similarity'], marker='o')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Plot of territoriality depending on years')\n",
    "\n",
    "# Add labels to the axes (optional)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('territoriality')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_functional_sim=issue_ownership(model, topicwords=filtered_TERR_FUNC_LEXICON[1], smooth=False, boot=True)\n",
    "plt.plot(corpus_functional_sim['year'], corpus_functional_sim['similarity'], marker='o')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Plot of functionality depending on years')\n",
    "\n",
    "# Add labels to the axes (optional)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('functionality')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_trends=pd.DataFrame()\n",
    "corpus_trends['year']=corpus_functional_sim['year']\n",
    "corpus_trends['functionality']=corpus_functional_sim['similarity']\n",
    "corpus_trends['territoriality']=corpus_territorial_sim['similarity']\n",
    "plt.plot(corpus_trends['year'], corpus_trends['functionality'], marker='o', label='functionality')\n",
    "plt.plot(corpus_trends['year'], corpus_trends['territoriality'], marker='o', color='r', label='territoriality')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('Plot of functionality and territoriality depending on years')\n",
    "\n",
    "# Add labels to the axes (optional)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('functionality and territoriality')\n",
    "\n",
    "plt.legend()\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "#je veux high territorial a droite et high functional en haut\n",
    "#Chat GPT :\n",
    "territorial_related_words = ['border','countryside','landscape','province','island',\n",
    "                             'peninsula','terrain','wilderness','topography','continent', \n",
    "                              'state','empire','kingdom','sovereignty','homeland','federation',\n",
    "                            'rural', 'urban', 'city', ]\n",
    "functional_related_words = ['globalization','capitalism','technology','economy','industry','digital',\n",
    "                            'market','finance','innovation','trade']\n",
    "TERR_FUNC_LEXICON=[ territorial_related_words+territorial_terms, functional_related_words+functional_terms]\n",
    "\n",
    "filtered_TERR_FUNC_LEXICON = [[word for word in sublist if word in model.wv] for sublist in TERR_FUNC_LEXICON]\n",
    "\n",
    "print(filtered_TERR_FUNC_LEXICON)\n",
    "\n",
    "def linear_projection_1D(pVec, vecXLeft, vecXRight):    \n",
    "    vecX = vecXRight.mean(axis=0) - vecXLeft.mean(axis=0) \n",
    "    return np.dot(pVec, vecX)  \n",
    "\n",
    "def linear_projection_2D(pVec, vecXLeft, vecXRight, vecYDown, vecYUp):    \n",
    "    vecX = vecXRight.mean(axis=0) - vecXLeft.mean(axis=0) \n",
    "    vecY = vecYUp.mean(axis=0) - vecYDown.mean(axis=0)\n",
    "    return (np.dot(pVec, vecX), np.dot(pVec, vecY)) \n",
    "\n",
    "def get_vector(model, words, M):\n",
    "    words = [w for w in words if w in model.wv.vocab]\n",
    "    L = len(words)\n",
    "    temp = np.zeros((L, M))\n",
    "    for i, x in enumerate(words):\n",
    "        temp[i,:] = model.wv[x]\n",
    "    return temp\n",
    "\n",
    "def custom_projection_1D(z, model, custom_lexicon=None):\n",
    "    M = model.vector_size\n",
    "    if custom_lexicon:\n",
    "        lex = custom_lexicon\n",
    "        if len(lex)!=2:\n",
    "            raise ValueError(\"The custom lexicon should be a list of lists, with two elements.\")\n",
    "    else:\n",
    "        lex = [filtered_TERR_FUNC_LEXICON[0], filtered_TERR_FUNC_LEXICON[1]] \n",
    "    xl, xr = [get_vector(model, words, M) for words in lex] \n",
    "    projections = [linear_projection_1D(x, xl, xr) for x in z]\n",
    "    Z = np.array(projections) \n",
    "    return Z\n",
    "\n",
    "\n",
    "def get_vector(model, words, M):\n",
    "    words_in_vocab = [w for w in words if w in model.wv]\n",
    "    words_not_in_vocab = [w for w in words if w not in model.wv]\n",
    "    if words_not_in_vocab:\n",
    "        print(f\"Words not in vocabulary: {words_not_in_vocab}\")\n",
    "    L = len(words_in_vocab)\n",
    "    temp = np.zeros((L, M))\n",
    "    for i, x in enumerate(words_in_vocab):\n",
    "        temp[i,:] = model.wv[x]\n",
    "    return temp\n",
    "def linear_projection_2D(pVec, vecXLeft, vecXRight, vecYDown, vecYUp):\n",
    "    if vecXLeft.size == 0 or vecXRight.size == 0:\n",
    "        print(\"Warning: vecXLeft or vecXRight is empty.\")\n",
    "    if vecYDown.size == 0 or vecYUp.size == 0:\n",
    "        print(\"Warning: vecYDown or vecYUp is empty.\")\n",
    "\n",
    "    vecX = np.nanmean(vecXRight, axis=0) - np.nanmean(vecXLeft, axis=0)\n",
    "    vecY = np.nanmean(vecYUp, axis=0) - np.nanmean(vecYDown, axis=0)\n",
    "    return (np.dot(pVec, vecX), np.dot(pVec, vecY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = data['UN_region'].values\n",
    "\n",
    "# Définir la taille de la police de caractère\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "# Trier les données par année\n",
    "df_sorted = data_embed.sort_values(by=['year'])\n",
    "\n",
    "# Définir la taille du graphique\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for un_region in set(data_embed['UN_region'].values):\n",
    "    # Filtrer les données pour la famille actuelle\n",
    "    data_region = data_embed.loc[data_embed['UN_region']==un_region].reset_index()\n",
    "    \n",
    "    # Obtenir les projections 1D pour les embeddings de la famille actuelle\n",
    "    projections_1D = custom_projection_1D(data_region[f'text_embedding'], model, custom_lexicon=TERR_FUNC_LEXICON)\n",
    "    \n",
    "    # Créer un graphique en nuage de points pour la famille actuelle\n",
    "    ax.scatter(projections_2D[:,0], projections_2D[:,1], alpha=0.7, label=un_region)\n",
    "    \n",
    "    # Ajouter l'année sur chaque point, avec une barre de séparation en cas de chevauchement\n",
    "    for i, before_after_tag in enumerate(data_region['year']):\n",
    "        # Vérifier s'il y a un chevauchement avec l'année précédente\n",
    "        if i > 0 and data_region['year'][i-1] == year:\n",
    "            # Ajouter une barre de séparation\n",
    "            ax.plot([projections_2D[i-1,0], projections_2D[i,0]], [projections_2D[i-1,1], projections_2D[i,1]], color=REGION_COL[family], linestyle='-', linewidth=0.5)\n",
    "            # Ajouter l'année avec une légère décalage vertical\n",
    "            ax.text(projections_2D[i,0], projections_2D[i,1]+0.8, year, ha='center', va='bottom')\n",
    "        else:\n",
    "            # Ajouter l'année sans barre de séparation\n",
    "            ax.text(projections_2D[i,0], projections_2D[i,1]+0.5, year, ha='center', va='bottom')\n",
    "\n",
    "    \n",
    "# Ajouter un titre et des labels d'axes\n",
    "# Définir la taille de la police de caractère pour le titre du graphique\n",
    "\n",
    "#ax.set_title('Party Placement in the European Parliament (1999-2022)', fontsize=14)\n",
    "\n",
    "# Définir la taille de la police de caractère pour les étiquettes d'axes\n",
    "ax.set_xlabel('Component 1', fontsize=12)\n",
    "ax.set_ylabel('Component 2', fontsize=12)\n",
    "\n",
    "\n",
    "# Ajouter une légen\n",
    "\n",
    "\n",
    "\n",
    "# Ajouter une légende\n",
    "ax.legend()\n",
    "plt.savefig('PCA_R_L_FR.png')\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('tree')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
